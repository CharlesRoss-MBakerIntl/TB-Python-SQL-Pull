{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction, Transformation, and Loading: PostgreSQL to AWS S3\n",
    "\n",
    "This notebook provides a step-by-step guide to perform the necessary steps to extract data from a PostgreSQL database, transform it, and load it into an Amazon S3 bucket. Specifically, you will:\n",
    "\n",
    "1. **Connect to the PostgreSQL Database**: Utilize the `psycopg2` library to establish a connection to the database and execute SQL queries to retrieve data.\n",
    "\n",
    "2. **Convert SQL Query Results into a Pandas DataFrame**: Leverage the `pandas` library to convert the retrieved data into a DataFrame for easier handling and manipulation.\n",
    "\n",
    "3. **Transform the Data**: Apply ETL (Extract, Transform, Load) operations on the DataFrame to clean and prepare the data for further use.\n",
    "\n",
    "4. **Upload the Transformed Data to S3**: Use the `boto3` library to send the prepared data to an Amazon S3 bucket for storage.\n",
    "\n",
    "This notebook serves as a practical tool to guide you through these steps, ensuring a seamless data extraction, transformation, and loading process. Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries and Packages\n",
    "\n",
    "Before we begin with the data extraction and transformation process, we need to load the necessary libraries and packages. This section will ensure that we have all the tools required to connect to the PostgreSQL database, manipulate the data, and interact with AWS S3.\n",
    "\n",
    "\n",
    "### Checking and Installing Required Packages\n",
    "To ensure that our environment has all the necessary packages for this notebook, we will first check if each package is installed. If any of the required packages are missing, we will download and install them. This step is crucial to avoid any interruptions during our data extraction, transformation, and loading process. Run the cell below by hitting **Control + Enter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Function to install packages\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    'requests',\n",
    "    'traceback',\n",
    "    'os',\n",
    "    'boto3',\n",
    "    'pandas',\n",
    "    'dotenv',\n",
    "    'datetime',\n",
    "    'psycopg2'\n",
    "]\n",
    "\n",
    "# Checking and installing packages\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found. Installing...\")\n",
    "        install_package(package)\n",
    "\n",
    "print(\"All required packages are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Packages\n",
    "\n",
    "- **requests**: A library for making HTTP requests to interact with web services and APIs.\n",
    "- **traceback**: A module for printing or retrieving a stack traceback, useful for debugging and error handling.\n",
    "- **os**: A module that provides a way to interact with the operating system, allowing us to access environment variables, file paths, and more.\n",
    "- **boto3**: The AWS SDK for Python, which allows us to interact with Amazon Web Services (AWS), such as S3, EC2, and more.\n",
    "- **pandas**: A powerful data manipulation and analysis library that provides data structures like DataFrames for handling and analyzing data.\n",
    "- **dotenv**: A library for reading key-value pairs from a `.env` file and loading them into the environment, useful for managing configuration and credentials.\n",
    "- **datetime**: A module for working with dates and times, enabling us to manipulate and format date and time data.\n",
    "- **psycopg2**: A PostgreSQL adapter for Python that allows us to connect to a PostgreSQL database and execute SQL queries.\n",
    "\n",
    "Now let's import these libraries and packages to ensure we have everything we need for our data processing workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import traceback\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages and Functions from Local Python Files\n",
    "\n",
    "In this section, we will import any additional packages and custom functions from local Python files. This step ensures that all necessary modules and functions specific to our project are available for use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_package import set_project\n",
    "from query_package import build_query_package\n",
    "\n",
    "from rds_connection import connect_rds\n",
    "from rds_processor import RDS\n",
    "\n",
    "from s3_bucket_util import update_active_data\n",
    "from s3_bucket_util import add_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Environment Variables\n",
    "\n",
    "In this section, we will explain how to load environment variables from a `.env` file. These variables are used to securely store sensitive information such as tokens, usernames, passwords, and paths that your application requires to function. This approach helps to keep such details out of your source code, enhancing security and making it easier to manage configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Environment Variables\n",
    "access = os.getenv('ACCESS')    # AWS access token\n",
    "secret = os.getenv('SECRET')    # AWS secret token\n",
    "username = os.getenv('USER')    # AWS RDS username\n",
    "password = os.getenv('PASS')    #AWS RDS password\n",
    "server = os.getenv('SERVER')    # AWS RDS server\n",
    "db = os.getenv('DB')    # AWS RDS database\n",
    "bucket = os.getenv('BUCKET')    # AWS RDS bucket name\n",
    "project_folder = os.getenv('PROJECTFOLDER')     # AWS S3 Project Folder Path\n",
    "active_folder = os.getenv('ACTIVEFOLDER')    # AWS S3 Project Active Folder Path\n",
    "archive_folder = os.getenv('ARCHIVEFOLDER')     # AWS S3 Project Archive Folder Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming the SQL Query\n",
    "\n",
    "## Input Project and Questions Information\n",
    "\n",
    "To begin, we need to gather the necessary project details and specific questions we aim to answer. This information will help us build an effective SQL query to retrieve the required data from the PostgreSQL database.\n",
    "\n",
    "### Selecting the Project\n",
    "The first step is to select the project you are working on. This can either be **Helene** or **Milton**. Depending on the project selected, the structure and requirements of the SQL query may vary. Make sure to choose the project that aligns with your current focus to ensure accurate data retrieval and analysis.\n",
    "Once the project is selected, we will proceed to define the specific questions or objectives that the SQL query aims to address. This will guide the formulation of the query and ensure that the retrieved data is relevant and useful for our analysis.\n",
    "\n",
    "> **Note:** If project names or codes need to be updated, this must be done in the Python file called `query_package` in the folder. The section that must be updated is located near the top and is called `project_dictionary`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = set_project(\"Helene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Questions\n",
    "\n",
    "Below, you will find the format for each question that you will want to pull from the SQL Database. Each question will result in a field in the overall table. This format ensures that all necessary information is included for building the SQL query. To add more questions, simply copy and paste the blank first question into the list in the cell below.\n",
    "\n",
    "#### Explanation of Dictionary Entries\n",
    "\n",
    "- **name**: This specifies what we want to name the field in the final dataset and will be used for joining when performing the query. Names of fields are recommended to be all lower case with spaces replaced by **\"_\"**. For example, 'hotel_name' or 'room_rate'.\n",
    "  \n",
    "- **single_or_repeat**: Indicates whether the question pertains to a single-entry ('SINGLE') or repeated entries ('REPEAT') within the PostgreSQL database.\n",
    "  \n",
    "- **data_source**: Refers to the table source within the PostgreSQL database where the question data can be found. For example, an answer containing integers will be stored in **application_data_numberanswer**.\n",
    "  \n",
    "- **question_id**: A unique identifier for the question, typically an integer that corresponds to the specific question in the database.\n",
    "  \n",
    "- **fields**: This specifies how we will rename the field to its official name when finally added into the overall dataset. The left side of the pair needs to be the name of the field stored in the question table within the **data_source** section, and the other side will be the name of the field we want in our final table. For example, in the **application_data_textboxanswer** table in the PostgreSQL database, answers are stored in a field called **value**. Therefore, **value** will be on the left side of the pair, but when we add this to the final dataset, we want a more unique name, so we add the desired field name to the right side of the pair. When we get a DataFrame back, the field is now named **hotel_name**. This must be done for all fields that we want to pull from the data source table in the PostgreSQL database. View the two examples below, and be sure to notice the difference in the fields section between the two questions.  It is recommended that you name this field after the name of the join in the first part of the package.\n",
    "\n",
    "\n",
    "\n",
    "#### EXAMPLE\n",
    "```python\n",
    "questions = [\n",
    "    {\n",
    "        'name': 'hotel_name',\n",
    "        'single_or_repeat': 'SINGLE',\n",
    "        'data_source': 'application_data_textboxanswer',\n",
    "        'question_id': 1015,\n",
    "        'fields': [{'value': 'hotel_name'}]\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': 'hotel_address',\n",
    "        'single_or_repeat': 'REPEATING',\n",
    "        'question_id': 1016,\n",
    "        'data_source': 'application_data_addressanswer',\n",
    "        'fields': {\n",
    "            'line1': 'hotel_address_line_1',\n",
    "            'line2': 'hotel_address_line_2',\n",
    "            'city': 'hotel_city',\n",
    "            'state': 'hotel_state',\n",
    "            'zip': 'hotel_zip'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Out the Question List\n",
    "\n",
    "To fill out the questions package, simply fill out each section with the information needed.  If more questions are needed, place a comm behind the closing **\"}\"**, then copy and paste a blank package of information below, make sure not to place it below the **\"]\"** at the very bottom of the cell. Repeat untill you have the total list of fields you want pulled from the PostgreSQL database. After you are done, hit **CONTROL + ENTER** to finalize.  In thi cell below the questions package is an example for pulling a database that focuses on hotel information.  You can use this as a reference while building out your own questions package.  Hit the **Click to Expand** to view this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \n",
    "    {\n",
    "        'name': '',\n",
    "        'single_or_repeat': '',\n",
    "        'data_source': '',\n",
    "        'question_id': 0,\n",
    "        'fields': [{'': ''}]\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': '',\n",
    "        'single_or_repeat': '',\n",
    "        'data_source': '',\n",
    "        'question_id': 0,\n",
    "        'fields': [{'': ''}]\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': '',\n",
    "        'single_or_repeat': '',\n",
    "        'data_source': '',\n",
    "        'question_id': 0,\n",
    "        'fields': [{'': ''}]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMPLE HOTEL SUMMARIES QUESTION LIST\n",
    "\n",
    "<details> <summary>Click to expand</summary>\n",
    "\n",
    "```python\n",
    "questions = [\n",
    "    \n",
    "    {'name': 'hotel_name',\n",
    "    'single_or_repeat': 'SINGLE',\n",
    "    'data_source': 'application_data_textboxanswer',\n",
    "    'question_id': 1015,\n",
    "    'fields' : [{'value':'hotel_name'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'hotel_address',\n",
    "    'single_or_repeat': 'REPEATING',\n",
    "    'question_id': 1016,\n",
    "    'data_source': 'application_data_addressanswer',\n",
    "    'fields': [{'line1':'hotel_address_line_1',\n",
    "                'line2': 'hotel_address_line_2',\n",
    "                'city' : 'hotel_city',\n",
    "                'state': 'hotel_state',\n",
    "                'zip' : 'hotel_zip'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'hotel_status',\n",
    "    'single_or_repeat': 'REPEATING',\n",
    "    'data_source': 'application_data_singleselectanswer',\n",
    "    'question_id': 1013,\n",
    "    'fields': [{'value':'hotel_status'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'license_in',\n",
    "    'single_or_repeat': 'REPEATING',\n",
    "    'data_source': 'application_data_dateanswer',\n",
    "    'question_id': 1021,\n",
    "    'fields': [{'value':'license_in'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'license_out',\n",
    "    'single_or_repeat': 'REPEATING',\n",
    "    'data_source': 'application_data_dateanswer',\n",
    "    'question_id': 1022,\n",
    "    'fields': [{'value':'license_out'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'total_in_household',\n",
    "    'single_or_repeat': 'SINGLE',\n",
    "    'data_source': 'application_data_numberanswer',\n",
    "    'question_id': 596,\n",
    "    'fields': [{'value':'total_in_household'}]\n",
    "    },\n",
    "\n",
    "\n",
    "    {'name': 'active_bookings',\n",
    "    'single_or_repeat': 'SINGLE',\n",
    "    'data_source': 'application_data_numberanswer',\n",
    "    'question_id': 1010,\n",
    "    'fields': [{'value':'active_bookings'}]\n",
    "    },\n",
    "\n",
    "    {'name': 'pathway_determination',\n",
    "    'single_or_repeat': 'SINGLE',\n",
    "    'data_source': 'application_data_singleselectanswer',\n",
    "    'question_id': 632,\n",
    "    'fields': [{'value':'pathway_determination'}]\n",
    "    }\n",
    "]\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 20px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Query\n",
    "\n",
    "Now we'll put all of this together, run the cell below to build the entire query package that we will be sending to the postgres database.  After the function is run, you can view the entire package by running the cell that is labeled **View Package**.  Remember, we need both the project and the questions in order build the query, if you haven't hit **CONTROL + ENTER** on these cells, its time to do it now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_package = build_query_package(project, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VIEW PACKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to PostgreSQL Database and Creating a Pandas DataFrame\n",
    "\n",
    "In this section, we will take the recently created query package and connect to our PostgreSQL database. Using the package, we will pull data from the database and convert it into a pandas DataFrame for further analysis.\n",
    "\n",
    "### Steps to Connect and Retrieve Data\n",
    "\n",
    "1. **Establish Database Connection**\n",
    "   - Next, we will establish a connection to our PostgreSQL database using the connection credentials.\n",
    "\n",
    "2. **Execute Query and Retrieve Data**\n",
    "   - Using the query package, we will execute SQL queries to pull data from the database.\n",
    "\n",
    "3. **Convert Data to Pandas DataFrame**\n",
    "   - Finally, we will convert the retrieved data into a pandas DataFrame for ease of data manipulation and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the PostgreSQL Database\n",
    "\n",
    "Below we'll use a function in the **rds_connection.py** file that will use the username, password, database name, and server named stored in the enviroment file to acess the PostgreSQL database.  Run the cell below to connect, if an error is returned, review the credentials in the .env file and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cursor = connect_rds(username, password, db, server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDS Connector Tool and Pull Dataframe\n",
    "\n",
    "Now we'll pull something called the RDS Connector Tool from a local file called **rds_processor.py**, this is a packaged designed to take in the query package and connection to process and build the dataframe.  The table will be stored in an item called **df**. During the building of the table, duplicates will be removed, you can view any of the duplicates by calling the **duplicate** item.  If the table is empty, then no duplicates were found.  You can view the dataframe by hitting **CONTROL + ENTER** on the cell labeled **VIEW DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Connector\n",
    "rds = RDS(conn, cursor, query_package) \n",
    "\n",
    "# Store the Dataframe\n",
    "df = rds.df\n",
    "\n",
    "# Store Duplicates\n",
    "duplicates = rds.duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VIEW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Modification Section\n",
    "\n",
    "In this section, you can modify the DataFrame as needed. Use the cells below to perform any necessary transformations, cleaning, or manipulations. Ensure that the final form of your DataFrame is saved as `df` for consistency and further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########    PERFORM ETL WITH PANDAS    #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DataFrame to S3 Bucket\n",
    "\n",
    "In this final step, we will load the processed DataFrame to an AWS S3 bucket. This process involves saving the DataFrame as a file in the specified S3 bucket's active folder, and additionally making a copy of this file in the archive folder. This ensures that the most recent data is available in the active location, while also maintaining a historical record of previous data versions in the archive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store S3 Bucket Information\n",
    "\n",
    "In this section you will find the S3 Bucket that we will be sumitting the dataframe to and storing that information for later use.  To do this you will need the name of the project folder you are trying to submit to followed by a **\"/\"**.  You will also need to enter the name of the bucket, as of right now, it should always be **reporting-external**, but if this changes, it will have to be updated below.  Also before this step, make sure that you have correctly set up the S3 Bucket to receive data, if you havent done this, follow the tutorial **Set Up S3 Bucket for Data Upload**\n",
    "\n",
    "\n",
    "##### EXAMPLE\n",
    "\n",
    "```python\n",
    "bucket = 'reporting-external'\n",
    "project_folder = 'Example_for_Tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Bucket Name\n",
    "bucket = 'reporting-external'\n",
    "\n",
    "#Set Project Folder\n",
    "project_folder = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Boto3 Client for S3\n",
    "\n",
    "In this section, we will create a Boto3 client for AWS S3. The Boto3 client is essential for interacting with S3 services, allowing us to upload, download, and manage data stored in S3 buckets.  Credentials for this connection is stored in the .env file\n",
    "\n",
    "### Storing Credentials Securely\n",
    "\n",
    "To ensure that our AWS credentials (access key and secret key) are kept secure, we store them in a `.env` file. This approach prevents sensitive information from being hard-coded into the source code, enhancing the security of our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boto3 client for S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=access,\n",
    "    aws_secret_access_key=secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DataFrame to Active S3 Bucket\n",
    "\n",
    "In this section, we will take the final DataFrame from the previous step and load it into a folder within an S3 bucket. The function `update_active_data` will be used for this purpose. Here’s a breakdown of the parameters being passed into the function:\n",
    "\n",
    "- **s3**: This is the Boto3 client for S3, which allows us to interact with the AWS S3 service.\n",
    "- **bucket**: The name of the S3 bucket where the data will be uploaded.\n",
    "- **project_folder**: The main project folder within the S3 bucket, which organizes your data.\n",
    "- **active_folder**: The specific subfolder within the project folder where the active data will be stored. In this case, it is set to `'Active/'`.\n",
    "- **file_name**: The name of the file that will be created in the active folder. This name can include dynamic elements such as the project folder name.\n",
    "- **data**: The DataFrame containing the data to be uploaded. This should be the final form of the DataFrame, saved as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_active_data(\n",
    "    s3 = s3, \n",
    "    bucket = bucket, \n",
    "    project_folder = project_folder, \n",
    "    active_folder = 'Active/', \n",
    "    file_name = f\"Active-{project_folder}.csv\", \n",
    "    data = df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 10px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Archive to S3 Bucket\n",
    "\n",
    "In this section, we will take a copy of the DataFrame and load it into an archive folder within the S3 bucket. The archive will maintain a maximum of 50 records in a rolling fashion, meaning new entries beyond 50 will replace the oldest ones. This setup ensures that you always have access to the most recent data while retaining a historical record up to a specified limit.\n",
    "\n",
    "The function `add_archive` will be used for this purpose. Here’s a breakdown of the parameters being passed into the function:\n",
    "\n",
    "- **s3**: This is the Boto3 client for S3, which allows us to interact with the AWS S3 service.\n",
    "- **bucket**: The name of the S3 bucket where the data will be archived.\n",
    "- **project_folder**: The main project folder within the S3 bucket, which organizes your data.\n",
    "- **archive_folder**: The specific subfolder within the project folder where the archived data will be stored.\n",
    "- **limit**: The maximum number of records to be retained in the archive. Once this limit is reached, older records will be replaced by newer ones.\n",
    "- **versions**: This represents the archive history, such as `rds.archive`, which keeps track of previous data versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_archive(\n",
    "    s3 = s3, \n",
    "    bucket = bucket, \n",
    "    project_folder = project_folder, \n",
    "    archive_folder = archive_folder, \n",
    "    limit = 50,\n",
    "    versions = rds.archive\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidal_basin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
